<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-01-26</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-01-26</h1>
        <p class="meta">Generated on 2026-01-26 09:35:34</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.16866v1" target="_blank">Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Lucía Güitta-López, Vincenzo Suriani, Jaime Boal, Álvaro J. López-López, Daniele Nardi
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-23 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.669</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Deep Reinforcement Learning (DRL) is a powerful framework for solving complex sequential decision-making problems, particularly in robotic control. However, its practical deployment is often hindered by the substantial amount of experience required for learning, which results in high computational a...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.16866v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.16866v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.16212v1" target="_blank">Point Bridge: 3D Representations for Cross Domain Policy Learning</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-22 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.655</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by th...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.16212v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.16212v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.15039v1" target="_blank">CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jiyao Zhang, Zhiyuan Ma, Tianhao Wu, Zeyuan Chen, Hao Dong
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-21 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.641</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.15039v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.15039v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.16046v1" target="_blank">DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Junha Lee, Eunha Park, Minsu Cho
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-22 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.613</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reaso...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.16046v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.16046v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.16207v1" target="_blank">IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jongwoo Park, Kanchana Ranasinghe, Jinhyeok Jang, Cristina Mata, Yoo Sung Jang, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-22 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.605</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the mod...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.16207v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.16207v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.15545v1" target="_blank">A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhifan Yan, Chang Liu, Yiyang Jiang, Wenxuan Zheng, Xinhao Chen, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-22 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.604</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 3)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suf...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.15545v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.15545v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.15541v1" target="_blank">CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Heng Zhang, Wei-Hsing Huang, Qiyi Tong, Gokhan Solak, Puze Liu, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-21 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.588</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.15541v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.15541v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.13639v1" target="_blank">A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Deyun Qin, Zezhi Liu, Hanqian Luo, Xiao Liang, Yongchun Fang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-20 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.580</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motio...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.13639v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.13639v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.13809v2" target="_blank">DroneVLA: VLA based Aerial Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-20 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.570</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting h...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.13809v2" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.13809v2" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.14874v1" target="_blank">HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yara Mahmoud, Yasheerah Yaqoot, Miguel Altamirano Cabrera, Dzmitry Tsetserukou
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-21 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.570</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-app...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.14874v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.14874v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
