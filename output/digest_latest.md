# arXiv Paper Digest - 2026-01-13

*Generated on 2026-01-13 16:34:05*

Found 10 relevant papers.

---

## 1. [Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model](http://arxiv.org/abs/2601.07695v1)

**Authors:** Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, et al. (+5 more)

**Published:** 2026-01-12

**Categories:** cs.CV

**Relevance Score:** 0.450

**Topic:** Computer Vision

**Abstract:** Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effect...

[PDF](https://arxiv.org/pdf/2601.07695v1) | [arXiv](http://arxiv.org/abs/2601.07695v1)

---

## 2. [Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction](http://arxiv.org/abs/2601.05491v1)

**Authors:** Luca Nunziante, Kentaro Uno, Gustavo H. Diaz, Shreya Santra, Alessandro De Luca, et al. (+1 more)

**Published:** 2026-01-09

**Categories:** cs.RO

**Relevance Score:** 0.443

**Topic:** Robotics Manipulation

**Abstract:** Since the successful Apollo program, humanity is once again aiming to return to the Moon for scientific discovery, resource mining, and inhabitation. Upcoming decades focus on building a lunar outpost, with robotic systems playing a crucial role to safely and efficiently establish essential infrastr...

[PDF](https://arxiv.org/pdf/2601.05491v1) | [arXiv](http://arxiv.org/abs/2601.05491v1)

---

## 3. [Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations](http://arxiv.org/abs/2601.03875v1)

**Authors:** Yuyang Fu, Xiuzhen Guo, Ji Shi

**Published:** 2026-01-07

**Categories:** eess.IV, cs.CV

**Relevance Score:** 0.443
 (Keyword matches: 2)

**Topic:** Computer Vision

**Abstract:** Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological...

[PDF](https://arxiv.org/pdf/2601.03875v1) | [arXiv](http://arxiv.org/abs/2601.03875v1)

---

## 4. [Robotic Tele-Operation for Upper Aerodigestive Tract Microsurgery: System Design and Validation](http://arxiv.org/abs/2601.06617v1)

**Authors:** Giovani Braglia, Jos√© Jair Alves Mendes Junior, Augusto Tetsuo Prado Inafuco, Federico Mariano, Leonardo S. Mattos

**Published:** 2026-01-10

**Categories:** cs.RO

**Relevance Score:** 0.441

**Topic:** Robotics Manipulation

**Abstract:** Upper aerodigestive tract (UADT) treatments frequently employ transoral laser microsurgery (TLM) for procedures such as the removal of tumors or polyps. In TLM, a laser beam is used to cut target tissue, while forceps are employed to grasp, manipulate, and stabilize tissue within the UADT. Although ...

[PDF](https://arxiv.org/pdf/2601.06617v1) | [arXiv](http://arxiv.org/abs/2601.06617v1)

---

## 5. [CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM](http://arxiv.org/abs/2601.03956v1)

**Authors:** Kangjie Zhou, Zhejia Wen, Zhiyong Zhuo, Zike Yan, Pengying Wu, et al. (+7 more)

**Published:** 2026-01-07

**Categories:** cs.RO

**Relevance Score:** 0.440

**Topic:** Robotics Manipulation

**Abstract:** Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigati...

[PDF](https://arxiv.org/pdf/2601.03956v1) | [arXiv](http://arxiv.org/abs/2601.03956v1)

---

## 6. [MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation](http://arxiv.org/abs/2601.06874v1)

**Authors:** Changli Wu, Haodong Wang, Jiayi Ji, Yutian Yao, Chunsai Du, et al. (+3 more)

**Published:** 2026-01-11

**Categories:** cs.CV

**Relevance Score:** 0.439

**Topic:** Computer Vision

**Abstract:** Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation ...

[PDF](https://arxiv.org/pdf/2601.06874v1) | [arXiv](http://arxiv.org/abs/2601.06874v1)

---

## 7. [CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos](http://arxiv.org/abs/2601.04061v1)

**Authors:** Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, et al. (+3 more)

**Published:** 2026-01-07

**Categories:** cs.RO, cs.CV

**Relevance Score:** 0.437

**Topic:** Robotics Manipulation

**Abstract:** Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation sk...

[PDF](https://arxiv.org/pdf/2601.04061v1) | [arXiv](http://arxiv.org/abs/2601.04061v1)

---

## 8. [VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control](http://arxiv.org/abs/2601.05138v1)

**Authors:** Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan, et al. (+1 more)

**Published:** 2026-01-08

**Categories:** cs.CV

**Relevance Score:** 0.434

**Topic:** Computer Vision

**Abstract:** Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aw...

[PDF](https://arxiv.org/pdf/2601.05138v1) | [arXiv](http://arxiv.org/abs/2601.05138v1)

---

## 9. [3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation](http://arxiv.org/abs/2601.04404v1)

**Authors:** Jusheng Zhang, Yijia Fan, Zimo Wen, Jian Wang, Keze Wang

**Published:** 2026-01-07

**Categories:** cs.CV, cs.AI

**Relevance Score:** 0.427

**Topic:** Computer Vision

**Abstract:** Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively W...

[PDF](https://arxiv.org/pdf/2601.04404v1) | [arXiv](http://arxiv.org/abs/2601.04404v1)

---

## 10. [LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model](http://arxiv.org/abs/2601.05248v1)

**Authors:** Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, et al. (+8 more)

**Published:** 2026-01-08

**Categories:** cs.RO

**Relevance Score:** 0.426

**Topic:** Robotics Manipulation

**Abstract:** Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. Howev...

[PDF](https://arxiv.org/pdf/2601.05248v1) | [arXiv](http://arxiv.org/abs/2601.05248v1)

---
