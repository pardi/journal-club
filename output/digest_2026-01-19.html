<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-01-19</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-01-19</h1>
        <p class="meta">Generated on 2026-01-19 09:35:05</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.10930v1" target="_blank">Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhixian Xie, Yu Xiang, Michael Posa, Wanxin Jin
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-16 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.720</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to real...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.10930v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.10930v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.11266v1" target="_blank">Skill-Aware Diffusion for Generalizable Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Aoshen Huang, Jiaming Chen, Jiyu Cheng, Ran Song, Wei Pan, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-16 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.697</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.11266v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.11266v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.08325v1" target="_blank">ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhenyang Liu, Yongchong Gu, Yikai Wang, Xiangyang Xue, Yanwei Fu
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-13 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.642</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Recent advances in robot manipulation have leveraged pre-trained vision-language models (VLMs) and explored integrating 3D spatial signals into these models for effective action prediction, giving rise to the promising vision-language-action (VLA) paradigm. However, most existing approaches overlook...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.08325v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.08325v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.10268v1" target="_blank">The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Eszter Birtalan, Mikl√≥s Koller
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-15 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.616</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.10268v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.10268v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.09920v1" target="_blank">SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-14 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.602</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.09920v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.09920v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.11076v1" target="_blank">A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-16 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.597</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throug...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.11076v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.11076v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.09988v1" target="_blank">In-the-Wild Compliant Manipulation with UMI-FT</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Hojung Choi, Yifan Hou, Chuer Pan, Seongheon Hong, Austin Patel, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-15 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.580</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.09988v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.09988v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.09031v1" target="_blank">Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Xuetao Li, Wenke Huang, Mang Ye, Jifeng Xuan, Bo Du, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-13 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.576</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Humanoid robot manipulation is a crucial research area for executing diverse human-level tasks, involving high-level semantic reasoning and low-level action generation. However, precise scene understanding and sample-efficient learning from human demonstrations remain critical challenges, severely h...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.09031v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.09031v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.08034v1" target="_blank">Fiducial Exoskeletons: Image-Centric Robot State Estimation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Cameron Smith, Basile Van Hoorick, Vitor Guizilini, Yue Wang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.567</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We introduce Fiducial Exoskeletons, an image-based reformulation of 3D robot state estimation that replaces cumbersome procedures and motor-centric pipelines with single-image inference. Traditional approaches - especially robot-camera extrinsic estimation - often rely on high-precision actuators an...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.08034v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.08034v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.10827v1" target="_blank">Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Simin Liu, Tong Zhao, Bernhard Paus Graesdal, Peter Werner, Jiuguang Wang, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-15 |
                <strong>Categories:</strong> cs.RO, cs.AI, eess.SY
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.556</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> If we consider human manipulation, it is clear that contact-rich manipulation (CRM)-the ability to use any surface of the manipulator to make contact with objects-can be far more efficient and natural than relying solely on end-effectors (i.e., fingertips). However, state-of-the-art model-based plan...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.10827v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.10827v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
