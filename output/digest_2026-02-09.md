# arXiv Paper Digest - 2026-02-09

*Generated on 2026-02-09 10:03:52*

Found 10 relevant papers.

---

## 1. [AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping](http://arxiv.org/abs/2602.03547v1)

**Authors:** Dingyi Zhou, Mu He, Zhuowei Fang, Xiangtong Yao, Yinlong Liu, et al. (+2 more)

**Published:** 2026-02-03

**Categories:** cs.RO, cs.CV

**Relevance Score:** 0.691
 (Keyword matches: 3)

**Topic:** Robotics Manipulation

**Abstract:** We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more cont...

[PDF](https://arxiv.org/pdf/2602.03547v1) | [arXiv](http://arxiv.org/abs/2602.03547v1)

---

## 2. [MobileManiBench: Simplifying Model Verification for Mobile Manipulation](http://arxiv.org/abs/2602.05233v1)

**Authors:** Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang, et al. (+3 more)

**Published:** 2026-02-05

**Categories:** cs.RO

**Relevance Score:** 0.674
 (Keyword matches: 1)

**Topic:** Robotics Manipulation

**Abstract:** Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce ...

[PDF](https://arxiv.org/pdf/2602.05233v1) | [arXiv](http://arxiv.org/abs/2602.05233v1)

---

## 3. [MultiGraspNet: A Multitask 3D Vision Model for Multi-gripper Robotic Grasping](http://arxiv.org/abs/2602.06504v1)

**Authors:** Stephany Ortuno-Chanelo, Paolo Rabino, Enrico Civitelli, Tatiana Tommasi, Raffaello Camoriano

**Published:** 2026-02-06

**Categories:** cs.RO, cs.CV

**Relevance Score:** 0.668
 (Keyword matches: 1)

**Topic:** Robotics Manipulation

**Abstract:** Vision-based models for robotic grasping automate critical, repetitive, and draining industrial tasks. Existing approaches are typically limited in two ways: they either target a single gripper and are potentially applied on costly dual-arm setups, or rely on custom hybrid grippers that require ad-h...

[PDF](https://arxiv.org/pdf/2602.06504v1) | [arXiv](http://arxiv.org/abs/2602.06504v1)

---

## 4. [Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations](http://arxiv.org/abs/2602.06643v1)

**Authors:** Ruiqian Nai, Boyuan Zheng, Junming Zhao, Haodong Zhu, Sicong Dai, et al. (+6 more)

**Published:** 2026-02-06

**Categories:** cs.RO, cs.AI, cs.LG

**Relevance Score:** 0.653
 (Keyword matches: 1)

**Topic:** Robotics Manipulation

**Abstract:** Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to con...

[PDF](https://arxiv.org/pdf/2602.06643v1) | [arXiv](http://arxiv.org/abs/2602.06643v1)

---

## 5. [RoboPaint: From Human Demonstration to Any Robot and Any View](http://arxiv.org/abs/2602.05325v1)

**Authors:** Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang, et al. (+2 more)

**Published:** 2026-02-05

**Categories:** cs.RO

**Relevance Score:** 0.628

**Topic:** Robotics Manipulation

**Abstract:** Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, envi...

[PDF](https://arxiv.org/pdf/2602.05325v1) | [arXiv](http://arxiv.org/abs/2602.05325v1)

---

## 6. [Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation](http://arxiv.org/abs/2602.04243v1)

**Authors:** Pengfei Yi, Yifan Han, Junyan Li, Litao Liu, Wenzhao Lian

**Published:** 2026-02-04

**Categories:** cs.RO

**Relevance Score:** 0.621
 (Keyword matches: 1)

**Topic:** Robotics Manipulation

**Abstract:** Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptabilit...

[PDF](https://arxiv.org/pdf/2602.04243v1) | [arXiv](http://arxiv.org/abs/2602.04243v1)

---

## 7. [GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning](http://arxiv.org/abs/2602.04315v1)

**Authors:** Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang

**Published:** 2026-02-04

**Categories:** cs.RO, cs.CV

**Relevance Score:** 0.619

**Topic:** Robotics Manipulation

**Abstract:** Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to ...

[PDF](https://arxiv.org/pdf/2602.04315v1) | [arXiv](http://arxiv.org/abs/2602.04315v1)

---

## 8. [Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation](http://arxiv.org/abs/2602.06512v1)

**Authors:** Junhong Zhu, Ji Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen

**Published:** 2026-02-06

**Categories:** cs.RO

**Relevance Score:** 0.619
 (Keyword matches: 2)

**Topic:** Robotics Manipulation

**Abstract:** While generalist robot policies hold significant promise for learning diverse manipulation skills through imitation, their performance is often hindered by the long-tail distribution of training demonstrations. Policies learned on such data, which is heavily skewed towards a few data-rich head tasks...

[PDF](https://arxiv.org/pdf/2602.06512v1) | [arXiv](http://arxiv.org/abs/2602.06512v1)

---

## 9. [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](http://arxiv.org/abs/2602.02454v1)

**Authors:** Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, et al. (+1 more)

**Published:** 2026-02-02

**Categories:** cs.RO, cs.AI

**Relevance Score:** 0.604
 (Keyword matches: 2)

**Topic:** Robotics Manipulation

**Abstract:** Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert d...

[PDF](https://arxiv.org/pdf/2602.02454v1) | [arXiv](http://arxiv.org/abs/2602.02454v1)

---

## 10. [Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF](http://arxiv.org/abs/2602.06834v1)

**Authors:** Allen Tao, Jun Yang, Stanko Oparnica, Wenjie Xue

**Published:** 2026-02-06

**Categories:** cs.RO

**Relevance Score:** 0.593

**Topic:** Robotics Manipulation

**Abstract:** Visual servoing is fundamental to robotic applications, enabling precise positioning and control. However, applying it to textureless objects remains a challenge due to the absence of reliable visual features. Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, le...

[PDF](https://arxiv.org/pdf/2602.06834v1) | [arXiv](http://arxiv.org/abs/2602.06834v1)

---
