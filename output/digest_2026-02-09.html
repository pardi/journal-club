<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-02-09</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-02-09</h1>
        <p class="meta">Generated on 2026-02-09 10:03:52</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2602.03547v1" target="_blank">AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Dingyi Zhou, Mu He, Zhuowei Fang, Xiangtong Yao, Yinlong Liu, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-03 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.691</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 3)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more cont...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.03547v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.03547v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2602.05233v1" target="_blank">MobileManiBench: Simplifying Model Verification for Mobile Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Wenbo Wang, Fangyun Wei, QiXiu Li, Xi Chen, Yaobo Liang, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-05 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.674</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-language-action models have advanced robotic manipulation but remain constrained by reliance on the large, teleoperation-collected datasets dominated by the static, tabletop scenes. We propose a simulation-first framework to verify VLA architectures before real-world deployment and introduce ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.05233v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.05233v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2602.06504v1" target="_blank">MultiGraspNet: A Multitask 3D Vision Model for Multi-gripper Robotic Grasping</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Stephany Ortuno-Chanelo, Paolo Rabino, Enrico Civitelli, Tatiana Tommasi, Raffaello Camoriano
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-06 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.668</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-based models for robotic grasping automate critical, repetitive, and draining industrial tasks. Existing approaches are typically limited in two ways: they either target a single gripper and are potentially applied on costly dual-arm setups, or rely on custom hybrid grippers that require ad-h...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.06504v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.06504v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2602.06643v1" target="_blank">Humanoid Manipulation Interface: Humanoid Whole-Body Manipulation from Robot-Free Demonstrations</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Ruiqian Nai, Boyuan Zheng, Junming Zhao, Haodong Zhu, Sicong Dai, et al. (+6 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-06 |
                <strong>Categories:</strong> cs.RO, cs.AI, cs.LG
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.653</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Current approaches for humanoid whole-body manipulation, primarily relying on teleoperation or visual sim-to-real reinforcement learning, are hindered by hardware logistics and complex reward engineering. Consequently, demonstrated autonomous skills remain limited and are typically restricted to con...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.06643v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.06643v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2602.05325v1" target="_blank">RoboPaint: From Human Demonstration to Any Robot and Any View</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jiacheng Fan, Zhiyue Zhao, Yiqian Zhang, Chao Chen, Peide Wang, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-05 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.628</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Acquiring large-scale, high-fidelity robot demonstration data remains a critical bottleneck for scaling Vision-Language-Action (VLA) models in dexterous manipulation. We propose a Real-Sim-Real data collection and data editing pipeline that transforms human demonstrations into robot-executable, envi...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.05325v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.05325v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2602.04243v1" target="_blank">Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Pengfei Yi, Yifan Han, Junyan Li, Litao Liu, Wenzhao Lian
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-04 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.621</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptabilit...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.04243v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.04243v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2602.04315v1" target="_blank">GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Guoqing Ma, Siheng Wang, Zeyu Zhang, Shan Yu, Hao Tang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-04 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.619</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.04315v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.04315v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2602.06512v1" target="_blank">Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Junhong Zhu, Ji Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-06 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.619</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> While generalist robot policies hold significant promise for learning diverse manipulation skills through imitation, their performance is often hindered by the long-tail distribution of training demonstrations. Policies learned on such data, which is heavily skewed towards a few data-rich head tasks...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.06512v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.06512v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2602.02454v1" target="_blank">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-02 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.604</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert d...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.02454v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.02454v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2602.06834v1" target="_blank">Perception-Control Coupled Visual Servoing for Textureless Objects Using Keypoint-Based EKF</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Allen Tao, Jun Yang, Stanko Oparnica, Wenjie Xue
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-02-06 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.593</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Visual servoing is fundamental to robotic applications, enabling precise positioning and control. However, applying it to textureless objects remains a challenge due to the absence of reliable visual features. Moreover, adverse visual conditions, such as occlusions, often corrupt visual feedback, le...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2602.06834v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2602.06834v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
