<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-01-13</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-01-13</h1>
        <p class="meta">Generated on 2026-01-13 17:32:41</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.05105v1" target="_blank">UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Filippo Ghilotti, Samuel Brucker, Nahku Saidy, Matteo Matteucci, Mario Bijelic, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.378</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by lever...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05105v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05105v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.07218v1" target="_blank">SceneNAT: Masked Generative Modeling for Language-Guided Indoor Scene Synthesis</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jeongjun Choi, Yeonsoo Park, H. Jin Kim
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.369</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We present SceneNAT, a single-stage masked non-autoregressive Transformer that synthesizes complete 3D indoor scenes from natural language instructions through only a few parallel decoding passes, offering improved performance and efficiency compared to prior state-of-the-art approaches. SceneNAT is...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07218v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07218v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.07830v1" target="_blank">Optimal Learning Rate Schedule for Balancing Effort and Performance</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Valentina Njaradi, Rodrigo Carrasco-Davis, Peter E. Latham, Andrew Saxe
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.LG, cs.NE, q-bio.NC
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.368</span>
                <span class="topic-tag">Machine Learning</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We int...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07830v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07830v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.05368v1" target="_blank">MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Svitlana Morkva, Maximum Wilder-Smith, Michael Oechsle, Alessio Tonioni, Marco Hutter, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.366</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate r...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05368v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05368v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.07119v1" target="_blank">SC-MII: Infrastructure LiDAR-based 3D Object Detection on Edge Devices for Split Computing with Multiple Intermediate Outputs Integration</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Taisuke Noguchi, Takayuki Nishio, Takuya Azumi
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.DC, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.365</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> 3D object detection using LiDAR-based point cloud data and deep neural networks is essential in autonomous driving technology. However, deploying state-of-the-art models on edge devices present challenges due to high computational demands and energy consumption. Additionally, single LiDAR setups suf...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07119v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07119v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.07519v1" target="_blank">Fast Multi-Stack Slice-to-Volume Reconstruction via Multi-Scale Unrolled Optimization</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Margherita Firenze, Sean I. Young, Clinton J. Wang, Hyuk Jin Yun, Elfar Adalsteinsson, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> eess.IV, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.358</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Fully convolutional networks have become the backbone of modern medical imaging due to their ability to learn multi-scale representations and perform end-to-end inference. Yet their potential for slice-to-volume reconstruction (SVR), the task of jointly estimating 3D anatomy and slice poses from mis...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07519v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07519v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.05116v1" target="_blank">From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zirui Wu, Zeren Jiang, Martin R. Oswald, Jie Song
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.355</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geome...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05116v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05116v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.05172v2" target="_blank">CoV: Chain-of-View Prompting for Spatial Reasoning</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Haoyu Zhao, Akide Liu, Zeyu Zhang, Weijie Wang, Feng Chen, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.353</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to ac...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05172v2" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05172v2" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.07640v1" target="_blank">Dual-Level Models for Physics-Informed Multi-Step Time Series Forecasting</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Mahdi Nasiri, Johanna Kortelainen, Simo Särkkä
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> stat.ML, cs.LG
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.351</span>
                <span class="topic-tag">Machine Learning</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> This paper develops an approach for multi-step forecasting of dynamical systems by integrating probabilistic input forecasting with physics-informed output prediction. Accurate multi-step forecasting of time series systems is important for the automatic control and optimization of physical processes...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07640v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07640v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.05251v1" target="_blank">Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.351</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire anim...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05251v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05251v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
