<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-01-13</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-01-13</h1>
        <p class="meta">Generated on 2026-01-13 15:46:28</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.03782v1" target="_blank">PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Wenlong Huang, Yu-Wei Chao, Arsalan Mousavian, Ming-Yu Liu, Dieter Fox, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.RO, cs.AI, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.663</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: gi...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.03782v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.03782v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.04629v1" target="_blank">UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhongxuan Li, Zeliang Guo, Jun Hu, David Navarro-Alarcon, Jia Pan, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.646</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We present UniBiDex a unified teleoperation framework for robotic bimanual dexterous manipulation that supports both VRbased and leaderfollower input modalities UniBiDex enables realtime contactrich dualarm teleoperation by integrating heterogeneous input devices into a shared control stack with con...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.04629v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.04629v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.07304v1" target="_blank">Heterogeneous Multi-Expert Reinforcement Learning for Long-Horizon Multi-Goal Tasks in Autonomous Forklifts</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yun Chen, Bowei Huang, Fan Guo, Kang Song
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.637</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Autonomous mobile manipulation in unstructured warehouses requires a balance between efficient large-scale navigation and high-precision object interaction. Traditional end-to-end learning approaches often struggle to handle the conflicting demands of these distinct phases. Navigation relies on robu...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07304v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07304v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.05243v1" target="_blank">Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Xingyi He, Adhitya Polavaram, Yunhao Cao, Om Deshmukh, Tianrui Wang, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.634</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models....
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05243v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05243v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.07060v1" target="_blank">PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yuanzhe Liu, Jingyuan Zhu, Yuchen Mo, Gen Li, Xu Cao, et al. (+7 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-11 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.629</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Recent advancements in vision-language-action (VLA) models have shown promise in robotic manipulation, yet they continue to struggle with long-horizon, multi-step tasks. Existing methods lack internal reasoning mechanisms that can identify task-relevant interaction cues or track progress within a su...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07060v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07060v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.07821v1" target="_blank">Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.RO, cs.AI, cs.LG
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.619</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07821v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07821v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.05844v1" target="_blank">DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yutong Liang, Shiyi Xu, Yulong Zhang, Bowen Zhan, He Zhang, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-09 |
                <strong>Categories:</strong> cs.GR, cs.AI, cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.617</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-cost visio...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05844v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05844v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.05499v1" target="_blank">TOSC: Task-Oriented Shape Completion for Open-World Dexterous Grasp Generation from Partial Point Clouds</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Weishang Wu, Yifei Shi, Zhiping Cai
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-09 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.607</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Task-oriented dexterous grasping remains challenging in robotic manipulations of open-world objects under severe partial observation, where significant missing data invalidates generic shape completion. In this paper, to overcome this limitation, we study Task-Oriented Shape Completion, a new task t...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05499v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05499v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.05241v1" target="_blank">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, et al. (+6 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV, cs.AI, cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.580</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-pro...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05241v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05241v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.04356v1" target="_blank">UNIC: Learning Unified Multimodal Extrinsic Contact Estimation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhengtong Xu, Yuki Shirai
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.RO, cs.AI, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.548</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, su...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.04356v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.04356v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
