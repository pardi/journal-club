<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-01-13</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-01-13</h1>
        <p class="meta">Generated on 2026-01-13 16:40:27</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.07377v1" target="_blank">Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jiao Xu, Xin Chen, Lihe Zhang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.CV, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.399</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> In this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel d...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07377v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07377v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.07474v1" target="_blank">Task Prototype-Based Knowledge Retrieval for Multi-Task Learning from Partially Annotated Data</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Youngmin Oh, Hyung-Il Kim, Jung Uk Kim
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.LG, cs.AI, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.398</span>
                <span class="topic-tag">Machine Learning</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Multi-task learning (MTL) is critical in real-world applications such as autonomous driving and robotics, enabling simultaneous handling of diverse tasks. However, obtaining fully annotated data for all tasks is impractical due to labeling costs. Existing methods for partially labeled MTL typically ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07474v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07474v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.06833v1" target="_blank">SPINE Gripper: A Twisted Underactuated Mechanism-based Passive Mode-Transition Gripper</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> JaeHyung Jang, JunHyeong Park, Joong-Ku Lee, Jee-Hwan Ryu
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-11 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.395</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> This paper presents a single-actuator passive gripper that achieves both stable grasping and continuous bidirectional in-hand rotation through mechanically encoded power transmission logic. Unlike conventional multifunctional grippers that require multiple actuators, sensors, or control-based switch...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.06833v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.06833v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.04137v1" target="_blank">Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Chun-Kai Fan, Xiaowei Chi, Xiaozhu Ju, Hao Li, Yong Bao, et al. (+16 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.RO, cs.AI, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.394</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still hav...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.04137v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.04137v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.05208v2" target="_blank">MoE3D: A Mixture-of-Experts Module for 3D Reconstruction</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zichen Wang, Ang Cao, Liam J. Wang, Jeong Joon Park
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.393</span>
                <span class="topic-tag">Computer Vision</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We propose a simple yet effective approach to enhance the performance of feed-forward 3D reconstruction models. Existing methods often struggle near depth discontinuities, where standard regression losses encourage spatial averaging and thus blur sharp boundaries. To address this issue, we introduce...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05208v2" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05208v2" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.07389v1" target="_blank">On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Xueyan Niu, Bo Bai, Wei Han, Weixi Zhang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.LG, cs.AI, cs.IT
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.389</span>
                <span class="topic-tag">Machine Learning</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Post-training of large language models routinely interleaves supervised fine-tuning (SFT) with reinforcement learning (RL). These two methods have different objectives: SFT minimizes the cross-entropy loss between model outputs and expert responses, while RL maximizes reward signals derived from hum...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07389v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07389v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.03784v1" target="_blank">A Comparative Study of 3D Model Acquisition Methods for Synthetic Data Generation of Agricultural Products</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Steven Moonen, Rob Salaets, Kenneth Batstone, Abdellatif Bey-Temsamani, Nick Michiels
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.388</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> In the manufacturing industry, computer vision systems based on artificial intelligence (AI) are widely used to reduce costs and increase production. Training these AI models requires a large amount of training data that is costly to acquire and annotate, especially in high-variance, low-volume manu...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.03784v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.03784v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.07516v1" target="_blank">Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.CL, cs.AI, cs.LG
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.387</span>
                <span class="topic-tag">Machine Learning</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalizatio...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07516v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07516v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.06496v1" target="_blank">3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Hao Tang, Ting Huang, Zeyu Zhang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-10 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.385</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it re...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.06496v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.06496v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.04052v1" target="_blank">Stable Language Guidance for Vision-Language-Action Models</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhihao Zhan, Yuhao Chen, Jiaying Zhou, Qinhan Lv, Hao Liu, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.RO, cs.CL
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.383</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-Language-Action (VLA) models have demonstrated impressive capabilities in generalized robotic control; however, they remain notoriously brittle to linguistic perturbations. We identify a critical ``modality collapse'' phenomenon where strong visual priors overwhelm sparse linguistic signals, ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.04052v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.04052v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
