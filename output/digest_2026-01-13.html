<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-01-13</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-01-13</h1>
        <p class="meta">Generated on 2026-01-13 16:34:05</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.07695v1" target="_blank">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, et al. (+5 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-12 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.450</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effect...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.07695v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.07695v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.05491v1" target="_blank">Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Luca Nunziante, Kentaro Uno, Gustavo H. Diaz, Shreya Santra, Alessandro De Luca, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-09 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.443</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Since the successful Apollo program, humanity is once again aiming to return to the Moon for scientific discovery, resource mining, and inhabitation. Upcoming decades focus on building a lunar outpost, with robotic systems playing a crucial role to safely and efficiently establish essential infrastr...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05491v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05491v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.03875v1" target="_blank">Staged Voxel-Level Deep Reinforcement Learning for 3D Medical Image Segmentation with Noisy Annotations</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yuyang Fu, Xiuzhen Guo, Ji Shi
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> eess.IV, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.443</span>
                <span class="topic-tag">Computer Vision</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Deep learning has achieved significant advancements in medical image segmentation. Currently, obtaining accurate segmentation outcomes is critically reliant on large-scale datasets with high-quality annotations. However, noisy annotations are frequently encountered owing to the complex morphological...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.03875v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.03875v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.06617v1" target="_blank">Robotic Tele-Operation for Upper Aerodigestive Tract Microsurgery: System Design and Validation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Giovani Braglia, Jos√© Jair Alves Mendes Junior, Augusto Tetsuo Prado Inafuco, Federico Mariano, Leonardo S. Mattos
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-10 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.441</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Upper aerodigestive tract (UADT) treatments frequently employ transoral laser microsurgery (TLM) for procedures such as the removal of tumors or polyps. In TLM, a laser beam is used to cut target tissue, while forceps are employed to grasp, manipulate, and stabilize tissue within the UADT. Although ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.06617v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.06617v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.03956v1" target="_blank">CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Kangjie Zhou, Zhejia Wen, Zhiyong Zhuo, Zike Yan, Pengying Wu, et al. (+7 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.440</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigati...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.03956v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.03956v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.06874v1" target="_blank">MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Changli Wu, Haodong Wang, Jiayi Ji, Yutian Yao, Chunsai Du, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-11 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.439</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.06874v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.06874v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.04061v1" target="_blank">CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.RO, cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.437</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Generalist Vision-Language-Action models are currently hindered by the scarcity of robotic data compared to the abundance of human video demonstrations. Existing Latent Action Models attempt to leverage video data but often suffer from visual entanglement, capturing noise rather than manipulation sk...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.04061v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.04061v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.05138v1" target="_blank">VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Sixiao Zheng, Minghao Yin, Wenbo Hu, Xiaoyu Li, Ying Shan, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.CV
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.434</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aw...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05138v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05138v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.04404v1" target="_blank">3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jusheng Zhang, Yijia Fan, Zimo Wen, Jian Wang, Keze Wang
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-07 |
                <strong>Categories:</strong> cs.CV, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.427</span>
                <span class="topic-tag">Computer Vision</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively W...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.04404v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.04404v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.05248v1" target="_blank">LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Zhuoyang Liu, Jiaming Liu, Hao Chen, Ziyu Guo, Chengkai Hou, et al. (+8 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-08 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.426</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. Howev...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.05248v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.05248v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
