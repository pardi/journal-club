<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>arXiv Paper Digest - 2026-02-02</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        .meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 30px;
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            background-color: #fafafa;
            border-left: 4px solid #3498db;
            border-radius: 4px;
        }
        .paper h2 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 1.3em;
        }
        .paper h2 a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper h2 a:hover {
            color: #3498db;
        }
        .paper-meta {
            color: #7f8c8d;
            font-size: 0.9em;
            margin: 10px 0;
        }
        .paper-meta strong {
            color: #555;
        }
        .abstract {
            margin: 15px 0;
            color: #555;
        }
        .links {
            margin-top: 15px;
        }
        .links a {
            display: inline-block;
            padding: 6px 12px;
            margin-right: 10px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 0.9em;
        }
        .links a:hover {
            background-color: #2980b9;
        }
        .score {
            display: inline-block;
            padding: 4px 8px;
            background-color: #2ecc71;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .topic-tag {
            display: inline-block;
            padding: 4px 8px;
            background-color: #9b59b6;
            color: white;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>arXiv Paper Digest - 2026-02-02</h1>
        <p class="meta">Generated on 2026-02-02 09:48:49</p>
        <p class="meta">Found 10 relevant papers.</p>
        <hr>

        <div class="paper">
            <h2>1. <a href="http://arxiv.org/abs/2601.22988v1" target="_blank">Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Di Zhang, Weicheng Duan, Dasen Gu, Hongye Lu, Hai Zhang, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-30 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.716</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Real-world robotic manipulation demands visuomotor policies capable of robust spatial scene understanding and strong generalization across diverse camera viewpoints. While recent advances in 3D-aware visual representations have shown promise, they still suffer from several key limitations, including...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.22988v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.22988v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>2. <a href="http://arxiv.org/abs/2601.23087v1" target="_blank">Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Wu Songwei, Jiang Zhiduo, Xie Guanghu, Liu Yang, Liu Hong
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-30 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.681</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 2)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Learning long-horizon robotic manipulation requires jointly achieving expressive behavior modeling, real-time inference, and stable execution, which remains challenging for existing generative policies. Diffusion-based approaches provide strong modeling capacity but typically incur high inference la...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.23087v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.23087v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>3. <a href="http://arxiv.org/abs/2601.21971v1" target="_blank">MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Lorenzo Mazza, Ariel Rodriguez, Rayan Younis, Martin Lelis, Ortrun Hellig, et al. (+4 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO, cs.AI, cs.LG
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.658</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) ...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.21971v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.21971v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>4. <a href="http://arxiv.org/abs/2601.22074v1" target="_blank">mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.616</span>
                <span class="topic-tag">Robotics Manipulation</span>

            </div>

            <div class="abstract">
                <strong>Abstract:</strong> We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, reward...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.22074v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.22074v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>5. <a href="http://arxiv.org/abs/2601.21713v1" target="_blank">Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Donatien Delehelle, Fei Chen, Darwin Caldwell
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO, cs.AI
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.608</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As ana...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.21713v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.21713v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>6. <a href="http://arxiv.org/abs/2601.21602v1" target="_blank">AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Jianli Sun, Bin Tian, Qiyao Zhang, Chengxiang Li, Zihan Song, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.600</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling betwe...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.21602v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.21602v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>7. <a href="http://arxiv.org/abs/2601.21394v1" target="_blank">Towards Space-Based Environmentally-Adaptive Grasping</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Leonidas Askianakis, Aleksandr Artemov
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.596</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Robotic manipulation in unstructured environments requires reliable execution under diverse conditions, yet many state-of-the-art systems still struggle with high-dimensional action spaces, sparse rewards, and slow generalization beyond carefully curated training scenarios. We study these limitation...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.21394v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.21394v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>8. <a href="http://arxiv.org/abs/2601.21129v1" target="_blank">WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Guangping Liu, Tipu Sultan, Vittorio Di Giorgio, Nick Hawkins, Flavio Esposito, et al. (+1 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.590</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately,...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.21129v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.21129v1" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>9. <a href="http://arxiv.org/abs/2601.20321v2" target="_blank">TaF-VLA: Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li, et al. (+3 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-28 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.579</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physic...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.20321v2" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.20321v2" target="_blank">arXiv Page</a>
            </div>
        </div>

        <div class="paper">
            <h2>10. <a href="http://arxiv.org/abs/2601.21712v1" target="_blank">CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation</a></h2>

            <div class="paper-meta">
                <strong>Authors:</strong> Xuanran Zhai, Binkai Ou, Yemin Wang, Hui Yi Leong, Qiaojun Yu, et al. (+2 more)
            </div>

            <div class="paper-meta">
                <strong>Published:</strong> 2026-01-29 |
                <strong>Categories:</strong> cs.RO
            </div>

            <div class="paper-meta">
                <span class="score">Relevance: 0.579</span>
                <span class="topic-tag">Robotics Manipulation</span>
 <em>(Keyword matches: 1)</em>
            </div>

            <div class="abstract">
                <strong>Abstract:</strong> Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that pre...
            </div>

            <div class="links">
                <a href="https://arxiv.org/pdf/2601.21712v1" target="_blank">PDF</a>
                <a href="http://arxiv.org/abs/2601.21712v1" target="_blank">arXiv Page</a>
            </div>
        </div>

    </div>
</body>
</html>
